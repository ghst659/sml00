{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e703d3e8-eed1-48f9-af6e-70d9a7acc2b6",
   "metadata": {},
   "source": [
    "# Text Classification from Scratch\n",
    "Reference https://keras.io/examples/nlp/text_classification_from_scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd1b44-050d-4888-95e4-138d1f325f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import typing\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.security.fuzzing.py.annotation_types as _atypes\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca3da6-ac6f-43b0-8135-cb93566a77df",
   "metadata": {},
   "source": [
    "## Fetch Data\n",
    "\n",
    "Data will come from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "\n",
    "In the shell:\n",
    "```\n",
    "$ mkdir /tmp/text\n",
    "$ pushd /tmp/text\n",
    "$ wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "$ tar -zxf aclImdb_v1.tar.gz\n",
    "$ ls aclImdb\n",
    "README  imdb.vocab  imdbEr.txt  test  train\n",
    "$ ls aclImdb/train\n",
    "labeledBow.feat  neg  pos  unsup  unsupBow.feat  urls_neg.txt  urls_pos.txt  urls_unsup.txt\n",
    "$ ls aclImdb/test\n",
    "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n",
    "```\n",
    "\n",
    "Text files are in folders for positive & negative reviews.\n",
    "* positive `aclImdb/train/pos`\n",
    "* negative `aclImdb/train/neg`\n",
    "\n",
    "Use [`keras.utils.text_dataset_from_directory`](https://keras.io/api/data_loading/text/) to generate a `tf.data.Dataset` object from texts in classified folders.  Only supports `.txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea52f92-4293-444f-8f3d-2899b496e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAROOT = \"/tmp/text/aclImdb\"  # the data extraction dir of the above shell commands.\n",
    "BATCH_SIZE = 32\n",
    "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(DATAROOT, \"train\"),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_val_ds = keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(DATAROOT, \"train\"),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_test_ds = keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(DATAROOT, \"test\"),\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ca02f-f12d-4fb7-8dd9-cf3048b66e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36bf43-dc8c-4dde-ad1c-5fdcf810a011",
   "metadata": {},
   "source": [
    "Data sample preview; the [`tf.data.Dataset.take(count)`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) method creates a `Dataset` with at most `count` elements from this dataset.\n",
    "\n",
    "The `raw_train` and `raw_val` datasets when iterated, yield (text, label) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c05490-7ce5-4ec2-9bdd-6ab1f7399882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's important to take a look at your raw data to ensure your normalization\n",
    "# and tokenization will work as expected. We can do that by taking a few\n",
    "# examples from the training set and looking at them.\n",
    "# This is one of the places where eager execution shines:\n",
    "# we can just evaluate these tensors using .numpy()\n",
    "# instead of needing to evaluate them in a Session/Graph context.\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf2147-8f01-4dcd-b6b2-f878f26259a0",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Remove `<br/>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14e4f3-2e21-4e71-8eb4-ff7d13e33458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data: typing.Annotated[typing.Any, _atypes.String]) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Having looked at our data above, we see that the raw text contains HTML break\n",
    "    tags of the form '<br />'. These tags will not be removed by the default\n",
    "    standardizer (which does not strip HTML). Because of this, we will need to\n",
    "    create a custom standardization function.\n",
    "    \"\"\"\n",
    "    lowercase = tf.strings.lower(input_data)  # https://www.tensorflow.org/api_docs/python/tf/strings/lower\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47891657-3b1e-46e2-91c3-615921363252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model constants\n",
    "MAX_FEATURES = 20000\n",
    "EMBEDDING_DIM = 128\n",
    "SEQUENCE_LENGTH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa79de2-6cef-405c-9611-bc3414559c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our custom standardization, we can instantiate our text\n",
    "# vectorization layer. We are using this layer to normalize, split, and map\n",
    "# strings to integers, so we set our 'output_mode' to 'int'.\n",
    "# Note that we're using the default split function,\n",
    "# and the custom standardization defined above.\n",
    "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
    "# model won't support ragged sequences.\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=MAX_FEATURES,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4eed11-c5d5-4d47-85f7-638cfc7919a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the vectorize_layer has been created, call `adapt` on a text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
    "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
    "\n",
    "# Let's make a text-only dataset (no labels):\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "# Let's call `adapt`:\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e3f4a-fad8-4446-b8a5-998a2b927bc5",
   "metadata": {},
   "source": [
    "## Two Ways to Vectorise Data\n",
    "\n",
    "1. **Part of the model** - the model processes raw strings.\n",
    "   ```\n",
    "   text_input = keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
    "   x = vectorize_layer(text_input)\n",
    "   x = layers.Embedding(max_features + 1, embedding_dim)(x)\n",
    "   ```\n",
    "\n",
    "2. **Apply Vectoriser to the Dataset** -- generates dataset of word indices, and model takes integer sequences.\n",
    "   Enables asynchronous CPU processing and buffering when training on a GPU.\n",
    "   ```\n",
    "   def vectorize_text(text, label):\n",
    "       text = tf.expand_dims(text, -1)\n",
    "       return vectorize_layer(text), label\n",
    "\n",
    "   # Vectorize the data.\n",
    "   train_ds = raw_train_ds.map(vectorize_text)\n",
    "   val_ds = raw_val_ds.map(vectorize_text)\n",
    "   test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "   # Do async prefetching / buffering of the data for best performance on GPU.\n",
    "   train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "   val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "   test_ds = test_ds.cache().prefetch(buffer_size=10)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ed1ee-0a44-4c60-acc3-9adb5db26d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5daa7ac-917a-4e6f-9aa4-5e0890872715",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "Using a 1-D convolution layer with an `Embedding` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2131f92-aadd-42ed-a007-cf53c3d609c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A integer input for vocab indices.\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "x = keras.layers.Embedding(MAX_FEATURES, EMBEDDING_DIM)(inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = keras.layers.Conv1D(EMBEDDING_DIM, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = keras.layers.Conv1D(EMBEDDING_DIM, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = keras.layers.Dense(EMBEDDING_DIM, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = keras.layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b905e0-4d12-45a6-a099-02b3ccd79af5",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8659fc7-5439-4400-996e-5d81222ae396",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eedb342-03a0-4d08-8447-b11c49fe5aa2",
   "metadata": {},
   "source": [
    "## Model Evaluation on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb2e8c-f1e2-464b-ad85-778e04b463af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c6c544-d1ba-422b-8820-25c03134793b",
   "metadata": {},
   "source": [
    "## Creating an End-to-End Model\n",
    "A model to process raw strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bce220-1a9b-434f-85c1-faafba83fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A string input\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "# Turn strings into vocab indices\n",
    "indices = vectorize_layer(inputs)\n",
    "# Turn vocab indices into predictions\n",
    "outputs = model(indices)\n",
    "\n",
    "# Our end to end model\n",
    "end_to_end_model = keras.Model(inputs, outputs)\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "end_to_end_model.evaluate(raw_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6371434c-4c4b-4c6e-a190-c23c1c77e25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
